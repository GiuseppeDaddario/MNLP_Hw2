Loading libxml2/2.10.3-zbbe7lm
  Loading requirement: xz/5.4.6-xxxg42c zlib-ng/2.1.6-jkgunjc
Loading libiconv/1.17-d7yvx2s
  ERROR: Module cannot be loaded due to a conflict.
    HINT: Might try "module unload libiconv" first.

Loading gettext/0.22.3-2g7elif
  ERROR: Load of requirement libiconv/1.17-d7yvx2s failed
Loading libiconv/1.17-d7yvx2s
  ERROR: Module cannot be loaded due to a conflict.
    HINT: Might try "module unload libiconv" first.

Loading gettext/0.22.3-2g7elif
  ERROR: Requirement libiconv/1.17-d7yvx2s is not loaded

Loading python/3.11.6--gcc--8.5.0
  ERROR: Load of requirement libiconv/1.17-d7yvx2s failed
  ERROR: Load of requirement gettext/0.22.3-2g7elif failed
Loading openmpi/4.1.6--gcc--12.2.0
  Loading requirement: gcc/12.2.0
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:41<01:22, 41.21s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:21<00:40, 40.43s/it]Loading checkpoint shards: 100%|██████████| 3/3 [02:00<00:00, 40.14s/it]Loading checkpoint shards: 100%|██████████| 3/3 [02:00<00:00, 40.29s/it]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 24 examples [00:00, 756.15 examples/s]
Map:   0%|          | 0/24 [00:00<?, ? examples/s]Map: 100%|██████████| 24/24 [00:00<00:00, 3707.26 examples/s]
Map:   0%|          | 0/50 [00:00<?, ? examples/s]Map: 100%|██████████| 50/50 [00:00<00:00, 6740.00 examples/s]
Filter:   0%|          | 0/1030 [00:00<?, ? examples/s]Filter: 100%|██████████| 1030/1030 [00:00<00:00, 29290.03 examples/s]
Map:   0%|          | 0/1030 [00:00<?, ? examples/s]Map: 100%|██████████| 1030/1030 [00:00<00:00, 12066.00 examples/s]
Map:   0%|          | 0/1104 [00:00<?, ? examples/s]Map:  10%|▉         | 107/1104 [00:00<00:00, 1054.12 examples/s]Map:  21%|██        | 234/1104 [00:00<00:00, 910.23 examples/s] Map:  32%|███▏      | 357/1104 [00:00<00:00, 858.79 examples/s]Map:  44%|████▍     | 491/1104 [00:00<00:00, 869.21 examples/s]Map:  55%|█████▍    | 602/1104 [00:00<00:00, 930.12 examples/s]Map:  64%|██████▎   | 702/1104 [00:00<00:00, 820.37 examples/s]Map:  72%|███████▏  | 793/1104 [00:01<00:00, 537.39 examples/s]Map:  78%|███████▊  | 866/1104 [00:01<00:00, 442.45 examples/s]Map:  88%|████████▊ | 976/1104 [00:01<00:00, 555.99 examples/s]Map:  97%|█████████▋| 1066/1104 [00:01<00:00, 552.33 examples/s]Map: 100%|██████████| 1104/1104 [00:01<00:00, 647.80 examples/s]
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
