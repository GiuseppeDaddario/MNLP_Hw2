Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.47s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.45s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.45s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.45s/it]
/leonardo/home/userexternal/gdaddari/mnlp/lib/python3.11/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
/leonardo/home/userexternal/gdaddari/mnlp/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
Map:   0%|          | 0/31323 [00:00<?, ? examples/s]Map:   3%|▎         | 1000/31323 [00:10<05:09, 98.08 examples/s]Map:   6%|▋         | 2000/31323 [00:20<04:53, 99.74 examples/s]Map:  10%|▉         | 3000/31323 [00:30<04:45, 99.31 examples/s]Map:  13%|█▎        | 4000/31323 [00:40<04:34, 99.68 examples/s]Map:  16%|█▌        | 5000/31323 [00:50<04:24, 99.53 examples/s]Map:  19%|█▉        | 6000/31323 [01:00<04:17, 98.47 examples/s]Map:  22%|██▏       | 7000/31323 [01:10<04:06, 98.52 examples/s]Map:  26%|██▌       | 8000/31323 [01:20<03:57, 98.34 examples/s]Map:  29%|██▊       | 9000/31323 [01:31<03:47, 97.99 examples/s]Map:  32%|███▏      | 10000/31323 [01:41<03:37, 98.25 examples/s]Map:  35%|███▌      | 11000/31323 [01:51<03:26, 98.57 examples/s]Map:  38%|███▊      | 12000/31323 [02:01<03:16, 98.28 examples/s]Map:  42%|████▏     | 13000/31323 [02:12<03:08, 97.29 examples/s]Map:  45%|████▍     | 14000/31323 [02:22<02:57, 97.53 examples/s]Map:  48%|████▊     | 15000/31323 [02:32<02:48, 97.01 examples/s]Map:  51%|█████     | 16000/31323 [02:43<02:38, 96.49 examples/s]Map:  54%|█████▍    | 17000/31323 [02:53<02:26, 97.54 examples/s]Map:  57%|█████▋    | 18000/31323 [03:03<02:17, 96.95 examples/s]Map:  61%|██████    | 19000/31323 [03:14<02:07, 96.97 examples/s]Map:  64%|██████▍   | 20000/31323 [03:24<01:56, 97.32 examples/s]Map:  64%|██████▍   | 20000/31323 [03:35<01:56, 97.32 examples/s]Map:  67%|██████▋   | 21000/31323 [03:37<01:56, 88.95 examples/s]Map:  70%|███████   | 22000/31323 [03:53<01:56, 80.34 examples/s]Map:  70%|███████   | 22000/31323 [04:05<01:56, 80.34 examples/s]Map:  73%|███████▎  | 23000/31323 [04:08<01:49, 75.70 examples/s]Map:  77%|███████▋  | 24000/31323 [04:22<01:38, 74.40 examples/s]Map:  77%|███████▋  | 24000/31323 [04:36<01:38, 74.40 examples/s]Map:  80%|███████▉  | 25000/31323 [04:36<01:26, 72.68 examples/s]Map:  83%|████████▎ | 26000/31323 [04:50<01:13, 72.29 examples/s]Map:  86%|████████▌ | 27000/31323 [05:06<01:01, 69.80 examples/s]Map:  86%|████████▌ | 27000/31323 [05:16<01:01, 69.80 examples/s]Map:  89%|████████▉ | 28000/31323 [05:21<00:48, 68.82 examples/s]Map:  93%|█████████▎| 29000/31323 [05:35<00:33, 69.15 examples/s]Map:  93%|█████████▎| 29000/31323 [05:46<00:33, 69.15 examples/s]Map:  96%|█████████▌| 30000/31323 [05:49<00:18, 70.09 examples/s]Map:  99%|█████████▉| 31000/31323 [06:02<00:04, 70.92 examples/s]Map: 100%|██████████| 31323/31323 [06:07<00:00, 71.50 examples/s]Map: 100%|██████████| 31323/31323 [06:07<00:00, 85.33 examples/s]
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
